{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f87220-fe7d-4d56-b441-f9bf457c98e4",
   "metadata": {},
   "source": [
    "Cleaned-up version of the literature review code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e5712-9886-420c-84dc-d11f1f88917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download required packages \n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409d14a-ab31-41ba-a0f6-aa3497caad67",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e660e0b-26a7-4195-825b-4169b51ac247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paperscraper, is python package licensed by MIT, \n",
    "#it lets you scrape publication metadata or full texts.\n",
    "from paperscraper.get_dumps import biorxiv, medrxiv, chemrxiv\n",
    "biorxiv()  # Takes ~1h and should result in ~350 MB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dee105-5af4-4756-b2eb-42f9696e5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show that json file is loaded correctly \n",
    "#confirm the existance of the server_dump folder and the .json1 file with the metadata for ~80000 papers. \n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_json('/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/biorxiv_2025-04-03.jsonl', lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bdbd5-a0bd-4930-876e-092e3d04ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paperscraper allows you to search for papers that have a combination of keywords different from the biorxiv API, so instead of using all possible keywords and filtering into categories later\n",
    "#I will search based on a combination of topics\n",
    "\n",
    "from paperscraper.xrxiv.xrxiv_query import XRXivQuery\n",
    "allkeywords= ['asst', 'dsbl','dsbi','cysteine', 'disulfide','cysj','cysi','cysh','cys','dsba', 'dsbb', 'h2s']\n",
    "\n",
    "#for all disulfide bond systems\n",
    "group1=['disulfide', 'bond', 'system'] \n",
    "#Asst/DsbL/DsbI + all relevant cysteine genes,\n",
    "group2=['asst']\n",
    "group3=['dsbi']\n",
    "group4=['dsbl']\n",
    "#Asst/DsbL/DsbI +pathogenicity/immunity, \n",
    "group5= ['h2s','immune']\n",
    "group6= ['h2s','pathogen']\n",
    "#cysJIH + pathogenicity/immunity, \n",
    "group7=['cys','pathogen']\n",
    "group8=['cys','immune']\n",
    "#cysJIH+ disulfide bond systems\n",
    "group9=['cys','disulfide']\n",
    "group10=['cysj']\n",
    "group11=['cysi']\n",
    "group12=['cysh']\n",
    "group13=['cysteine']\n",
    "\n",
    "#wanted to be able to see which words gave results, combining all makes it hard to know so i did it individually \n",
    "#query=[group1,group2,group3,group4,group5,group6,group7,group8,group9,group9,group10,group11,group12,group13]\n",
    "#querier.search_keywords(query, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/relevant_papers.jsonl')\n",
    "\n",
    "querier.search_keywords(group1, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group1.jsonl')\n",
    "querier.search_keywords(group2, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group2.jsonl')\n",
    "querier.search_keywords(group3, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group3.jsonl')\n",
    "querier.search_keywords(group4, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group4.jsonl')\n",
    "querier.search_keywords(group5, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group5.jsonl')\n",
    "querier.search_keywords(group6, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group6.jsonl')\n",
    "querier.search_keywords(group7, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group7.jsonl')\n",
    "querier.search_keywords(group8, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group8.jsonl')\n",
    "querier.search_keywords(group9, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group9.jsonl')\n",
    "querier.search_keywords(group10, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group10.jsonl')\n",
    "querier.search_keywords(group11, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group11.jsonl')\n",
    "querier.search_keywords(group12, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group12.jsonl')\n",
    "querier.search_keywords(group13, output_filepath='/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/group13.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c3f46-1401-456c-b6cd-f99532d43d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to combine all jsonl files into one so that I have a single file of interest.\n",
    "#Group10 did not result in any search results so had to delete that.\n",
    "# this code finds all the files that end with jsonl and combines them into one file \n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "folder_path = '/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/jsonfiles'\n",
    "\n",
    "# if it ends with jason you grab it\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.jsonl')]\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "# Iterate and append to a list \n",
    "for file in json_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)  # Load each line as a JSON object\n",
    "            combined_data.append(data)\n",
    "\n",
    "# Save as a new file\n",
    "output_file = '/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/jsonfiles/combined_file.jsonl'\n",
    "with open(output_file, 'w') as f:\n",
    "    for item in combined_data:\n",
    "        json.dump(item, f)  # Write each JSON object to a new line\n",
    "        f.write('\\n')  # Ensure each object is on a separate line\n",
    "\n",
    "print(f\"Combined JSONL files saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584eeae-0678-4898-85a5-790459e68c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many lines this file has\n",
    "#each line represents a paper \n",
    "file_path = '/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/jsonfiles/combined_file.jsonl'\n",
    "\n",
    "line_count = 0\n",
    "with open(file_path, 'r') as f:\n",
    "    line_count = sum(1 for line in f)  # This counts each line\n",
    "\n",
    "print(f\"The file has {line_count} lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0123e7c-cef0-4f4d-9474-927510a778da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 4233 individual papers from this search. \n",
    "#Now we need to download all of these pdf's using the doi numbers of each paper.\n",
    " #save pdfs\n",
    "from paperscraper.pdf import save_pdf_from_dump\n",
    "\n",
    "# Save PDFs/XMLs in current folder and name the files by their DOI\n",
    "save_pdf_from_dump('/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/jsonfiles/combined_file.jsonl', pdf_path='/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/papers', key_to_save='doi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e99c7-e4b9-4f65-86aa-a946571b5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how many pdf's are in the folder \n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = '/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/papers'\n",
    "num_items = len(os.listdir(folder_path))\n",
    "\n",
    "print(num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0434f46-14cb-4aab-b168-c6e5ea4481ce",
   "metadata": {},
   "source": [
    "There are 2594 files in the folder which makes sense because there were many replicates of the same papers since we searched for genes names that are in the same operon and are co-mentioned in papers. Now we need to make python read these papers and get the text out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d53bae-0163-48b4-9bdb-f796c6a2ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the pdf's into python using a different package\n",
    "#try pdfplumber\n",
    "\n",
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "folder_path = '/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/papers'\n",
    "pdf_text_dict = {}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"Extracting: {filename}\")\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            pdf_text_dict[filename] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f8829-8a00-4cf0-be31-67e744908f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show that there is actually a list of dictionaries with my text of interest \n",
    "print(len(pdf_text_dict))\n",
    "\n",
    "first_key = next(iter(pdf_text_dict))\n",
    "first_value = pdf_text_dict[first_key]\n",
    "\n",
    "print(f\"Filename: {first_key}\")\n",
    "print(f\"Text snippet: {first_value[:2000]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7f05d-8cd8-4a2b-8ddb-34ef42e6256d",
   "metadata": {},
   "source": [
    "pdfplumber was able to read every single paper except one (2593/2594). The code snipped above shows that the process went smoothly and the strings we got make sense (there are some special characters so we might need to deal with that later). Reading through all pdf files took 5+ hours so will save this dictionary as a json file as to not have to run this code again.\n",
    "\n",
    "If you look closely, the text also includes page numbers and the doi dislaimer the biorxiv papers have on top of every page, so it's not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187459a3-62e0-4b5f-93e6-6e153eb3f380",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_text_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_text_data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mpdf_text_dict\u001b[49m, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf_text_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#save this dictionary as a json file to not have to run the code again it takes hours \n",
    "import json\n",
    "\n",
    "with open(\"pdf_text_data.json\", \"w\") as f:\n",
    "    json.dump(pdf_text_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c74e34-9e7e-4efc-8a19-41dcc84ad81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-load pdf_text_dict after session expires\n",
    "\n",
    "import json\n",
    "with open(\"pdf_text_data.json\", \"r\") as f:\n",
    "    pdf_text_dict = json.load(f)\n",
    "\n",
    "# Check the loaded dictionary\n",
    "print(pdf_text_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da329e84-e7d3-458e-9e4a-99ccaff9aceb",
   "metadata": {},
   "source": [
    "Here is a quick EDA on how big these strings are and how the sizes are distributed. It's important to keep in mind that the length of these strings still include whtespace since we didn't process the strings in any way. There are a few outliers making this histogram right skewed, if not for the outliers the distribution would be pretty close to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9df60a-69f8-4d1f-a4af-6d09c7cbbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate and learn the lenght of each paper in characters\n",
    "import pandas as pd\n",
    "character_length=[]\n",
    "for key, item in pdf_text_dict.items():\n",
    "    character_length.append(len(item))\n",
    "character_length= pd.DataFrame(character_length, columns=['Character Lenght'])    \n",
    "\n",
    "#plot a histogram to visualize the sizes\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "g = sns.FacetGrid(character_length, height=4, aspect=1.5)\n",
    "g.map_dataframe(sns.histplot,x='Character Lenght')\n",
    "g.fig.suptitle(\"Distribution of Character Counts \", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b080e-f047-4524-b5fa-6410cde094f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words\n",
    "split_words = pd.DataFrame({\n",
    "    'Document': list(pdf_text_dict.keys()),\n",
    "    'Words': [text.split() for text in pdf_text_dict.values()]\n",
    "})\n",
    "\n",
    "#print(split_words.head())\n",
    "#number or words pr doc\n",
    "\n",
    "split_words['Word_Count'] = split_words['Words'].apply(len)\n",
    "\n",
    "#plot with histogram\n",
    "g = sns.FacetGrid(split_words, height=4, aspect=1.5)\n",
    "g.map_dataframe(sns.histplot, x='Word_Count')\n",
    "g.fig.suptitle(\"Distribution of Word Count\", fontsize=14)\n",
    "g.set_axis_labels(\"Word Count\", \"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a63e9-26e2-4485-b96f-7fcfeba0573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count sentences\n",
    "\n",
    "split_sentences = pd.DataFrame({\n",
    "    'Document': list(pdf_text_dict.keys()),\n",
    "    'Sentences': [text.split('.') for text in pdf_text_dict.values()]\n",
    "})\n",
    "\n",
    "#clean up sentences that are empty strings if there are any \n",
    "\n",
    "split_sentences['Sentences'] = split_sentences['Sentences'].apply(\n",
    "    lambda sents: [s.strip() for s in sents if s.strip()]\n",
    ")\n",
    "\n",
    "split_sentences['Sentence_Count'] = split_sentences['Sentences'].apply(len)\n",
    "#plot with histogram\n",
    "g = sns.FacetGrid(split_sentences, height=4, aspect=1.5)\n",
    "g.map_dataframe(sns.histplot, x='Sentence_Count')\n",
    "g.fig.suptitle(\"Distribution of Sentence Count\", fontsize=14)\n",
    "g.set_axis_labels(\"Word Count\", \"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b90add-fd23-428f-9c05-73845ece34e9",
   "metadata": {},
   "source": [
    "My ultimate goal is to cluster papers based on their content however it woud be very useful to have a log of relevant sentences that have words of interest so that I could look back on these notes and study for my PhD defence more thoroughly. For this part I want to be able to extract the sentence that has my word of interest and also extract +-1 sentence before it for context. I will save this information in an excel format so that it would be a lot easier to read manually for when I am studying for my exam. It will also come in handy when I need to find references for my thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca906b99-8953-45b5-8136-6e0759b16def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for words asst, dsbl, dsbi, disullfide ,h2s \n",
    "\n",
    "import re\n",
    "keyword = \"dsbl\"\n",
    "data = []\n",
    "\n",
    "#pdf_text_dict has all of the text in a dictionary (no additional filtering done)\n",
    "for doc_id, text in pdf_text_dict.items():\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        if keyword.lower() in sentences[i].lower():\n",
    "             #save the sentences before and after it \n",
    "            before = sentences[i - 1] if i > 0 else \"\"\n",
    "            match = sentences[i]\n",
    "            after = sentences[i + 1] if i < len(sentences) - 1 else \"\"\n",
    "            data.append({\n",
    "                \"Document\": doc_id,  #also keep doi number in case you want to find this paper again \n",
    "                \"Before\": before,\n",
    "                \"Match\": match,\n",
    "                \"After\": after\n",
    "            })\n",
    "\n",
    "# Save to excel for readability\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"dsbl_sentences.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886086e-d5f5-47c1-9763-137634b1ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for another keyword. \n",
    "keyword = \"cysJ\"\n",
    "data = []\n",
    "\n",
    "#pdf_text_dict has all of the text in a dictionary (no additional filtering done)\n",
    "for doc_id, text in pdf_text_dict.items():\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        if keyword.lower() in sentences[i].lower():\n",
    "             #save the sentences before and after it \n",
    "            before = sentences[i - 1] if i > 0 else \"\"\n",
    "            match = sentences[i]\n",
    "            after = sentences[i + 1] if i < len(sentences) - 1 else \"\"\n",
    "            data.append({\n",
    "                \"Document\": doc_id,  #also keep doi number in case you want to find this paper again \n",
    "                \"Before\": before,\n",
    "                \"Match\": match,\n",
    "                \"After\": after\n",
    "            })\n",
    "\n",
    "# Save to excel for readability\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(\"cysJ_sentences.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c733bf-aede-457e-a769-6e6eb47cf78a",
   "metadata": {},
   "source": [
    "I've already done RNAseq analysis (previous wet labe work done in the lab) on my E.coli strains of interest so it would be useful to know which E.coli genes are mentioned in these papers so that I could compare them to my RNAseq results. It's hard to determine novel genes of interest when you don't exactly know the relationship between these genes and especially when there is not that much published information about them, so I am hoping that seeing the names of the genes that are mentioned in these papers will help me understand the relationship between these two operons a lot better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffe71c-7bac-4ef6-b9c3-4dadfd764360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find gene names that are mentioned in these documents \n",
    "\n",
    "#this looks for 3 lower case +1 uppercase 4 letter word\n",
    "pattern = re.compile(r'\\b[a-z]{3}[A-Z]\\b')\n",
    "\n",
    "matches = []\n",
    "\n",
    "for doc_id, text in pdf_text_dict.items():\n",
    "    found_genes = pattern.findall(text)\n",
    "    if found_genes:\n",
    "        matches.append({\n",
    "            \"Document\": doc_id,\n",
    "            \"Matched Genes\": ', '.join(sorted(set(found_genes))) #removes the duplicates in a sentence and save with a comma in between\n",
    "        })\n",
    "\n",
    "# Save to Excel\n",
    "df = pd.DataFrame(matches)\n",
    "df.to_excel(\"matched_genes.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85751c7e-e55f-4db8-8777-60e158885de5",
   "metadata": {},
   "source": [
    "It's hard to go through each cell in excel (there are 900 + papers which matches) so I am going to plot a bar graph to see how frequently these genes are mentioned in these selection of papers. The histogram below show the frequency of mentiones from all papers combined an the repeated mentions in the same paper are not filtered this time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565147f-75ce-4634-8317-e63a406f9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "# need to count the number of genes that are mentioned for histogram \n",
    "from collections import Counter\n",
    "pattern = re.compile(r'\\b[a-z]{3}[A-Z]\\b')\n",
    "\n",
    "all_genes = []\n",
    "\n",
    "for doc_id, text in pdf_text_dict.items():\n",
    "    matches = pattern.findall(text)\n",
    "    all_genes.extend(matches)\n",
    "\n",
    "# count genes\n",
    "gene_counts = Counter(all_genes)\n",
    "\n",
    "# make a dataframe\n",
    "df_counts = pd.DataFrame(gene_counts.items(), columns=[\"Gene\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "#show frequency\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_counts[\"Gene\"], df_counts[\"Count\"])\n",
    "plt.xlabel(\"Gene Name\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Gene Name Matches\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f423cd-b018-428f-9bdc-fad411531fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are too many gene names to make a legible axis, show top 50 instead\n",
    "\n",
    "top_n = 50  #show top 50 instead \n",
    "df_top = df_counts.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_top[\"Gene\"], df_top[\"Count\"])\n",
    "plt.xlabel(\"Gene Name\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Top {top_n} Most Frequent Gene Name Matches\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc435312-4978-4ee6-9f25-53a977721c04",
   "metadata": {},
   "source": [
    "Before I move on to more sophisticated methods of clustering these papers, it is a good idea to manually categorize these papers based on keywords so that I could have a reference point to get back to. I had used very generic searchwords when initially downloading papers from Biorxiv, now I will use a combination of words (or/and) to filter more thoroughly. Rationally I expect these papers to cluster into 4 groups (topics), but the search words I used weren't very specific so there is likely a lot of non-specific papers that will not be clustred with the rest. I also expect some redundancy between the different groups since these genes are in the same operon and the papers are likely to co-mention similar topics.\n",
    "\n",
    "Here are the categories and the keywords I am interested in:\n",
    "\n",
    "disulfide bond system and pathogenicity= disulfide bond AssT or DsbL or DsbI and pathogen or immune or immunity\n",
    "h2s and cysJIH genes = H2S, hydrogen sulfide and cysJ cysI or cysH\n",
    "disulfide bond and cysJIH genes= disulfide bond AssT or DsbL or DsbI and cysJ cysI or cysH\n",
    "cysJIH and pathogenicity= cysJ cysI or cysH and pathogen or immune or immunity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2909ef-20ff-429a-8d5f-d7cb39841049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up groups of keywords \n",
    "bond= ['disulfide bond','AssT', 'DsbL', 'DsbI']\n",
    "h2s=['H2S','hydrojen sulfide','thiol']\n",
    "cys=['cysJ', 'cysI', 'cysH','cysM','cysK','tnaA','cysD','cysU','cysA','cysW', 'cysC', 'cysN','cysteine']\n",
    "pathogen=['pathogen', 'immune','immunity','response','innate']\n",
    "\n",
    "#group 1 disulfide bond system and pathogenicity\n",
    "group1={}\n",
    "\n",
    "for key, text in pdf_text_dict.items():\n",
    "    text_lower = text.lower()  # Case-insensitive match\n",
    "    has_bond = any(gene in text_lower for gene in bond)\n",
    "    has_pathogen = any(term in text_lower for term in pathogen)\n",
    "    \n",
    "    # Check if paper meets both conditions\n",
    "    if has_bond and has_pathogen:\n",
    "        group1[key] = text\n",
    "\n",
    "\n",
    "#group 2 h2s and cysJIH genes\n",
    "group2={}\n",
    "for key, text in pdf_text_dict.items():\n",
    "    text_lower = text.lower()  # Case-insensitive match\n",
    "    has_h2s = any(gene in text_lower for gene in h2s)\n",
    "    has_cys = any(term in text_lower for term in cys)\n",
    "    \n",
    "    # Check if paper meets both conditions\n",
    "    if has_h2s and has_cys:\n",
    "        group2[key] = text\n",
    "\n",
    "\n",
    "#group 3 disulfide bond and cysJIH genes\n",
    "group3={}\n",
    "for key, text in pdf_text_dict.items():\n",
    "    text_lower = text.lower()  # Case-insensitive match\n",
    "    has_bond2 = any(gene in text_lower for gene in bond)\n",
    "    has_cys2 = any(term in text_lower for term in cys)\n",
    "    \n",
    "    # Check if paper meets both conditions\n",
    "    if has_bond2 and has_cys2:\n",
    "        group3[key] = text\n",
    "\n",
    "#group 4 cysJIH and pathogenicity\n",
    "group4={}\n",
    "for key, text in pdf_text_dict.items():\n",
    "    text_lower = text.lower()  # Case-insensitive match\n",
    "    has_pathogen2 = any(gene in text_lower for gene in pathogen)\n",
    "    has_cys3 = any(term in text_lower for term in cys)\n",
    "    \n",
    "    # Check if paper meets both conditions\n",
    "    if has_pathogen2 and has_cys3:\n",
    "        group4[key] = text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98c83a-4186-468b-b663-d2881e890f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('disulfide bond system and pathogenicity papers: ' +str(len(group1)))\n",
    "print('h2s and cys gene papers: '+ str(len(group2)))\n",
    "print('disulfide bond and cysJIH genes: '+ str(len(group3)))\n",
    "print('cysJIH and pathogenicity '+ str(len(group4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7318e-1c74-4c23-9cba-bb304b1847e9",
   "metadata": {},
   "source": [
    "I got quite a few papers after filtering, however I did not get any matches with specific cys genes alone, I had to add the term cysteine to be able to get any matches, which confirms my suspicions that there isn't much research about the relationship between cys genes and the disulfide bond system or pathogenicity. How many of these papers are unique? I suspect that there are probaby a bunch that are repeated in the different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2f0e5-6daa-4c28-a6bc-89d52fc04fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique papers \n",
    "unique_papers={}\n",
    "\n",
    "for group in [group1, group2, group3, group4]:\n",
    "    unique_papers.update(group) #this will overwrite duplicates\n",
    "\n",
    "print('if you added up all groups it would make=  ' + str(len(group1)+len(group2)+len(group3)+ len(group4)))\n",
    "print('number of unique papers is= ' +str(len(unique_papers)))\n",
    "print('number of duplicate papers= '+ str(3358-1896))\n",
    "print('initial number of papers we downloaded prior to second filtering step:  '+ str(len(pdf_text_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3a7f2-9066-438d-ab3d-c972fed1ec65",
   "metadata": {},
   "source": [
    "As I guessed, a lot of these papers are repeats, in the end we got 1896 unique papers categorized into 4 categories, through manual categorization based on keyword search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b68a7-b8ca-4e4a-9ef3-f8e769713c72",
   "metadata": {},
   "source": [
    "Ultimately I decided to use the sliding window approach on the discussion sections alone. I had to trim unnecessary portions out of the papers to do this. For example most papers have an acknowlegments section after discussion and a bibliography, so I had to get rid of those since they would not have provided any useful information. One downside of this approach was I had to decide on a specific length to trim the discussion sections to. These papers all have different formats and different lengths so I had to make an educated guess and assume that 1500 words of discussion would be enough the reflect the findings of each paper. Not all papers had a 'Discussion' section, they instead used descriptive titles per section instead of dividing things up in a more traditional sense (i.e abstract, results, methods, discussion) so I had to filter those papers out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad374f3-2217-4897-a3e3-23820b3460e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sciBERT\n",
    "#download tokenizer and model\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb55ea8-a411-4c2c-8393-bad5a80d2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9cfa8-ccbd-4794-9c6c-db8a82a4a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the references section of the paragraphs so that you get more relevant info  \n",
    "import re\n",
    "removed_ref = {}\n",
    "\n",
    "for doc, text in pdf_text_dict.items():\n",
    "    #Clean up hyphenated line breaks and inline newlines\n",
    "    text = re.sub(r'-\\n', '', text)\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "\n",
    "    #try to include all possible section names (References/Bibliography/Citations)\n",
    "    split_text = re.split(r'\\b(References|Bibliography|Citations)\\b', text, flags=re.IGNORECASE) #NOT case sensitive\n",
    "    text = split_text[0] #this gets everything before the keyword \n",
    "\n",
    "    # Save cleaned text\n",
    "    removed_ref[doc] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e12ae-97c5-4c14-a915-5c8b1beaaa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show what the removed ref docs look like \n",
    "for doc, text in list(removed_ref.items())[:1]:  \n",
    "    print(f\"Document: {doc}\")\n",
    "    print(\"First few paragraphs:\", text[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e926d-c488-408d-b04a-c5934a361545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT THE DISCUSSION SECTION OF THE PAPERS\n",
    "#If there is no discussion, omit the paper \n",
    "\n",
    "discussion_dict = {}\n",
    "\n",
    "# Loop over your existing documents\n",
    "for doc, text in removed_ref.items():\n",
    "    # Split the text into two parts at the \"Discussion\" \n",
    "    split_text = re.split(r'\\bDiscussion\\b', text, flags=re.IGNORECASE)\n",
    "\n",
    "    if len(split_text) > 1:\n",
    "        #if discussion is found, keep section and add title\n",
    "        discussion_only = \"Discussion \" + split_text[1].strip()  # Keep only the part after \"Discussion\", add space after title\n",
    "        #add it to new dictionary\n",
    "        discussion_dict[doc] = discussion_only\n",
    "    else:\n",
    "        # if no discussion omit this document\n",
    "        continue\n",
    "\n",
    "    #save in case you need it\n",
    "    with open(\"discussion_text_checkpoint.pkl\", \"wb\") as f:\n",
    "        pickle.dump(discussion_dict, f)\n",
    "\n",
    "print(len(discussion_dict))\n",
    "\n",
    "# print to show you have the discussion section \n",
    "for doc, text in list(discussion_dict.items())[:2]:\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(\"First few lines of Discussion:\", text[-1000:])  #document doesn't end with discussion there is acknowledgements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b58685-621b-4c89-a4db-5f4189e3eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show that you are getting the discussion section \n",
    "doc, text = list(discussion_dict.items())[6] \n",
    "print(f\"Document: {doc}\")\n",
    "print(\"First few lines of Discussion:\", text[:1000]) #this gives 1000 characters not words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5efa02-cc97-4193-bbf9-d1febb6df240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE EVERYTHING AFTER ACKNOWLEDGEMENT, IF NOT PRESENT, KEEP TEXT\n",
    "#remove if there is supplemental information , or methods or anything else that is not relevant\n",
    "\n",
    "cleaned_discussion = {}\n",
    "\n",
    "for doc, text in discussion_dict.items():\n",
    "    # split it when you fins acknowledements, supplementaty informationm ,methods \n",
    "    split_text = re.split(r'\\b(Acknowledgements|Supplementary Information|Methods|Materials and Methods|Funding|Author Contributions)\\b', text, flags=re.IGNORECASE) #case insensitive\n",
    "\n",
    "    if len(split_text) > 1:\n",
    "        # if you find a header, keep the text before header\n",
    "        cleaned_text = split_text[0].strip()\n",
    "        cleaned_discussion[doc] = cleaned_text\n",
    "    else:\n",
    "        # If no headers, keep tet the way it is \n",
    "        cleaned_discussion[doc] = text\n",
    "\n",
    "print(len(cleaned_discussion))\n",
    "\n",
    "# Print the first few examples\n",
    "for doc, text in list(cleaned_discussion.items())[:6]:\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(\"How does discussion end:\", text[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902c5ba-675d-44d1-a1f8-549b6a5f19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split it into words before using sliding windows \n",
    "split_discussion = {}\n",
    "\n",
    "for doc, text in cleaned_discussion.items():\n",
    "    words = text.split()  # simple whitespace split\n",
    "    split_discussion[doc] = words\n",
    "\n",
    "# Show example\n",
    "doc, text = list(split_discussion.items())[5] \n",
    "print(f\"Document: {doc}\")\n",
    "print(f\"Split into words: {words[:1500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c45e8-d386-41df-aa4b-79eeea734ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the sliding window for the first 1500 characters of the discussion section \n",
    "\n",
    "def split_and_trim_to_1500_words(text):\n",
    "    #make sure it's split\n",
    "    words = text.split()\n",
    "    # If there are more than 1500 words, trim the list\n",
    "    if len(words) > 1500:\n",
    "        words = words[:1500]\n",
    "    return words\n",
    "    \n",
    "discussion_1500 = {}\n",
    "\n",
    "#iterate \n",
    "for doc, text in cleaned_discussion.items():\n",
    "    #use funtion to split\n",
    "    word_list = split_and_trim_to_1500_words(text)\n",
    "    # Add to new dictionary\n",
    "    discussion_1500[doc] = word_list\n",
    "\n",
    "# Show that it worked \n",
    "doc, word_list = list(discussion_1500.items())[7] \n",
    "print(f\"Document: {doc}\")\n",
    "print(\"1500 words:\", word_list[:200])\n",
    "print(len(word_list))\n",
    "\n",
    "#size is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093eeebc-b4d6-4cfe-bfea-87fad3fe9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply moving windows on discussion_1500 \n",
    "\n",
    "#you can have max 516 for sciBERT\n",
    "# Function to apply sliding window tokenization\n",
    "\n",
    "#scibert wants string not a list of words \n",
    "def sliding_window_tokenize(text, max_length=512, stride=128):\n",
    "\n",
    "    #need to combine words into single string again \n",
    "    #i already know the size is 1500 words for each\n",
    "    text = ' '.join(text)\n",
    "    input_ids = tokenizer(text, truncation=False, padding=False)['input_ids']\n",
    "    windows = []\n",
    "    for start in range(0, len(input_ids), stride):\n",
    "        end = min(start + max_length, len(input_ids))\n",
    "        windows.append(input_ids[start:end])\n",
    "        if end == len(input_ids):\n",
    "            break\n",
    "    return windows\n",
    "\n",
    "# Apply to each document in the dictionary\n",
    "windowed_docs = {}\n",
    "\n",
    "for doc_id, text in discussion_1500.items():\n",
    "    windowed_docs[doc_id] = sliding_window_tokenize(text, max_length=512, stride=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa806207-addd-4dc3-a317-a1aa4c0f9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the number of sliding windows \n",
    "#check that sliding windows worked \n",
    "#we got 5 windows for 1500 words \n",
    "\n",
    "for doc_id, windows in list(windowed_docs.items())[:3]:\n",
    "    print(f\"\\nDocument ID: {doc_id}\")\n",
    "    print(f\"Number of windows: {len(windows)}\")\n",
    "    for i, window in enumerate(windows):\n",
    "        print(f\"  Window {i+1} length: {len(window)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0d1ed-6dc0-410c-bc3b-ef7a8e35073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed windows\n",
    "\n",
    "def embed_windows2(windows, model, tokenizer, max_length=512, device='cpu'):\n",
    "    embeddings = []\n",
    "    \n",
    "    for input_ids in windows:\n",
    "        # Convert input_ids to tensor and move to the correct device\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  \n",
    "        \n",
    "        # Pad input_ids if necessary (SciBERT padding behavior)\n",
    "        padding_length = max_length - input_ids.shape[1]\n",
    "        if padding_length > 0:\n",
    "            padding = torch.full((1, padding_length), tokenizer.pad_token_id, dtype=torch.long).to(device)\n",
    "            input_ids = torch.cat([input_ids, padding], dim=1)\n",
    "        \n",
    "        # Ensure attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n",
    "        \n",
    "        # Feed the input_ids and attention_mask into the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            embeddings.append(outputs.last_hidden_state)  # [1, max_length, hidden_dim]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c506913-e3d6-43b6-9897-b1e1a9bc9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this on a smaller dataset to see if tokenizing is working \n",
    "windowed_embeddings2 = {}\n",
    "subset_docs = list(windowed_docs.items())[:100]  # Use the first 100 documents for testing\n",
    "for doc_id, windows in subset_docs:\n",
    "    embeddings = embed_windows2(windows, model, tokenizer, max_length=512, device='cpu')\n",
    "    windowed_embeddings2[doc_id] = embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c7ceb-d053-4045-abcc-c23c786a7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show contents \n",
    "#this works, every document is different \n",
    "doc, embeds = list(windowed_embeddings2.items())[1]\n",
    "print(f\"Contents of one window embedding: {embeds[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdec98-a1a5-4539-bb48-1e2385053e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save incrementally \n",
    "import os\n",
    "\n",
    "output_dir = '/Users/selinozgur/Desktop/Harvard/CSCI E-108 DATA MINING/project/embeddings/doc_embeddings/individual'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for doc_id, windows in windowed_docs.items():\n",
    "    embeddings = embed_windows2(windows, model, tokenizer, max_length=512, device='cpu')\n",
    "    \n",
    "    # Save each document's embeddings individually\n",
    "    with open(os.path.join(output_dir, f'{doc_id}.pkl'), 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "    \n",
    "    print(f'Saved: {doc_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc1175-6754-435c-b99f-d7f14ef3976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the individual embedding files\n",
    "\n",
    "combined_embeddings = {}\n",
    "\n",
    "for filename in os.listdir(output_dir):\n",
    "    doc_id = filename.replace('.pkl', '')\n",
    "    with open(os.path.join(output_dir, filename), 'rb') as f:\n",
    "        combined_embeddings[doc_id] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2a8ef-69d8-4969-9fc5-fbb0c541fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now pool the embeddings \n",
    "#take the average embedding over all 512 tokens per window \n",
    "#also pool per document\n",
    "\n",
    "doc_embeddings = []\n",
    "for window_embeds in combined_embeddings.values():  \n",
    "    #take mean\n",
    "    pooled = [embed.mean(dim=1) for embed in window_embeds]  \n",
    "    pooled = torch.cat(pooled, dim=0)  \n",
    "    doc_embedding = pooled.mean(dim=0)  \n",
    "    # neeed to convert to numpy for PCA\n",
    "    doc_embeddings.append(doc_embedding.numpy()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4084d1-2917-4b11-a1e3-9a13ea04660c",
   "metadata": {},
   "source": [
    "Even though I've calculated a single vector per paper from all of the moving windows, there are still a lot of dimensions (768) so it's probably a good idea to do dimensionality reduction before proceeding with the clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c0468-0eba-45a8-8434-fcff0d64e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a574d3e-8183-4380-8862-a816b132dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick how many components to reduce to \n",
    "#need to convert embedding dictionary to array or it doesn't work \n",
    "\n",
    "embedding_matrix = np.vstack(doc_embeddings)  \n",
    "\n",
    "#initiate pCA\n",
    "pca=PCA(n_components='mle') #mle decides on the optimum number of components \n",
    "fit_pca=pca.fit(embedding_matrix)\n",
    "\n",
    "#Print the cumulative sum of the variance \n",
    "cum_sum= np.cumsum(fit_pca.explained_variance_ratio_)\n",
    "print(cum_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956936a-e117-4e52-8828-4b4a3bbf723c",
   "metadata": {},
   "source": [
    "Baed on the cumsum table, the first variable explains about 30% of variance and it takes approximately 60 components to reach ~90%.After the first component the influence to variance goes down by quite a bit (less than 1 percent). I don't want to lose information but i also don't want to have too many dimensions so I will initially pick 50 components which explain about 80% of the variance .\n",
    "\n",
    "Below we can see how many n_components mle decided to pick (676). I initially tried working with 2400 components which is the max in case and also tried some manual values like 500 etc but mle decided on this value instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbace810-cfb6-46d6-a539-56fa74774a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the explained variance ratio vs. component \n",
    "#mle says best number for n_components is 767\n",
    "n_components = 767\n",
    "_=plt.scatter(range(1, n_components + 1), pca.explained_variance_ratio_[:n_components])    \n",
    "_=plt.hlines(0,1,n_components, color='red')\n",
    "_=plt.xlabel('Component number')\n",
    "_=plt.ylabel('Explain variance ratio')\n",
    "_=plt.title('Explained variance ratio vs. component number')\n",
    "\n",
    "tick_step = 99  # Show ticks every 100 components\n",
    "_=plt.xticks(np.arange(1, n_components + 1, step=tick_step), rotation=45)\n",
    "\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae37912-e137-4dea-97ea-65956c1e2ec0",
   "metadata": {},
   "source": [
    "The curve is consistent with what was seen in the table. As expected the first component contributes the most and it drops down drastically after that. 50 components look like a good number to start. I will reduce it to 50 components using PCA another time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e1dda-db16-4b7d-9893-cbbdf15095a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce to 50 \n",
    "\n",
    "#initiate pCA\n",
    "pca_50 = PCA(n_components=50) #I now know I want 50 components\n",
    "X50 = pca_50.fit_transform(embedding_matrix)   \n",
    "\n",
    "#Print the cumulative sum of the variance \n",
    "cum_sum_50= np.cumsum(pca_50.explained_variance_ratio_)\n",
    "print(cum_sum_50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c4eef-7cbf-4495-8a28-892fd2157c21",
   "metadata": {},
   "source": [
    "First trying out the OPTICS clustering model and UMAP visalization to see if I am getting good separation between potention clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541fcfc-b399-447f-b5ed-fdccbebe8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48e01d-1e7d-4ffb-a8be-71628904b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use optics on 50- Dimensioned reduced PCA data\n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "optics_model = OPTICS(min_samples=3, xi=0.02, min_cluster_size=0.01)\n",
    "optics_model.fit(X50)\n",
    "\n",
    "labels = optics_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34544f-0cee-4eaf-998b-f121e45c4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_2d = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "X2 = umap_2d.fit_transform(X50)         \n",
    "\n",
    "\n",
    "sns.scatterplot(x=X2[:, 0], y=X2[:, 1], hue=labels2, palette=\"tab10\", s=10, linewidth=0)\n",
    "plt.title(\"2D UMAP Projection of PCA(50) + OPTICS Clusters\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend(title=\"OPTICS Cluster Label\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a2090-8829-42f6-b364-4dfab8e89ae4",
   "metadata": {},
   "source": [
    "Cosine method thinks the majority of the big blob is just noise (-1) but it was able to find some distinct groups, for example cluster 3 and 7 for sure make sense and are different populations. Cosine is a better choice than euclidean for sciBERT embeddigs for sure. OPTICS is probably being very conservative so it's probably a good idea to try DBSCAN (again with cosine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef017ce7-8df9-438b-9525-34a7340166b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724ca81-1c36-420c-be0f-ef6d1291d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model = DBSCAN(eps= 0.4 , min_samples=3, metric='cosine')\n",
    "dbscan_model.fit(X50)\n",
    "\n",
    "labels_dbscan = dbscan_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167c5d6-f833-4653-a735-7d556c9dd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_2d = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42, metric='cosine')\n",
    "X2 = umap_2d.fit_transform(X50)\n",
    "\n",
    "eps_values = np.arange(0.15, 0.35, 0.01)\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, figsize=(25, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=3, metric='cosine')\n",
    "    labels = dbscan_model.fit_predict(X50)\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        x=X2[:, 0], y=X2[:, 1],\n",
    "        hue=labels,\n",
    "        palette='tab10',\n",
    "        s=10, linewidth=0,\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"DBSCAN (eps={eps})\")\n",
    "    axes[i].set_xlabel(\"UMAP 1\")\n",
    "    axes[i].set_ylabel(\"UMAP 2\")\n",
    "    axes[i].legend([], [], frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcd955-0f17-46f0-b995-7dfa975414f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase mini_samples from 3 to 6\n",
    "#also increased epcs to 0.25 it worked better with increased min_samples\n",
    "dbscan_model4 = DBSCAN(eps= 0.25 , min_samples=6, metric='cosine')\n",
    "dbscan_model4.fit(X50)\n",
    "\n",
    "labels_dbscan4 = dbscan_model4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfecf89c-990b-426e-9fad-60503fb3a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_2d = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "X2 = umap_2d.fit_transform(X50)         \n",
    "\n",
    "\n",
    "sns.scatterplot(x=X2[:, 0], y=X2[:, 1], hue=labels_dbscan4, palette=\"tab10\", s=10, linewidth=0)\n",
    "plt.title(\"2D UMAP Projection of PCA(50) + DBSCAN Clusters\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend(title=\"DBSCAN Cluster Label\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25366dd-bec8-4e95-bb7f-1c6ceefa6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the titles of clusters 1,5 and 7 \n",
    "#export clusters from model 4\n",
    "\n",
    "indices_1 = np.where(labels_dbscan4 == 1)[0]\n",
    "indices_5 = np.where(labels_dbscan4 == 5)[0]\n",
    "indices_7 = np.where(labels_dbscan4 == 7)[0]\n",
    "\n",
    "#find the dictionary that has the same indices but also still has the document ids\n",
    "\n",
    "doc_ids = list(combined_embeddings.keys())\n",
    "\n",
    "docs_cluster_1 = [doc_ids[i] for i in indices_1]\n",
    "docs_cluster_5 = [doc_ids[i] for i in indices_5]\n",
    "docs_cluster_7 = [doc_ids[i] for i in indices_7]\n",
    "\n",
    "\n",
    "print(docs_cluster_1[:5]) #This works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f354657-8130-4aec-b22e-9ae363d56b23",
   "metadata": {},
   "source": [
    " was able to get the documents ids for 3 of the clusters that I am interested in, but these are doi numbers and do not have the title names. Will need to search paper titles from the biorxiv downloads to match them to doi numbers. The document id's now end with '.pdf' since they were downloaded to my commputer, so I need to strip those before matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152d759-c8a5-4273-898c-55b5a09bb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for doi and match it to paper title\n",
    "#make it a dataframe\n",
    "\n",
    "#original database download with abstracts and title names\n",
    "#reload it \n",
    "\n",
    "doi_database = pd.read_json('/opt/anaconda3/lib/python3.12/site-packages/paperscraper/server_dumps/biorxiv_2025-04-03.jsonl', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1b373-94fb-4043-a865-f5e7794de556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#match them to title\n",
    "\n",
    "#clean .pdf\n",
    "docs_cluster_1_clean = [doc_id.replace('.pdf', '') for doc_id in docs_cluster_1]\n",
    "docs_cluster_5_clean = [doc_id.replace('.pdf', '') for doc_id in docs_cluster_5]\n",
    "docs_cluster_7_clean = [doc_id.replace('.pdf', '') for doc_id in docs_cluster_7]\n",
    "\n",
    "#show that the .pdfs are gone\n",
    "#print(docs_cluster_1_clean[:5])\n",
    "#print(docs_cluster_5_clean[:5])\n",
    "\n",
    "#match cluster1 titles\n",
    "cluster1_titles =pd .DataFrame(columns=['doi', 'title'])\n",
    "\n",
    "#doi database haas '/'instead of '_', need to fix that or no matches \n",
    "docs_cluster_1_normalized = [s.replace('_', '/') for s in docs_cluster_1_clean]\n",
    "\n",
    "cluster1_titles = doi_database[doi_database['doi'].isin(docs_cluster_1_normalized)][['doi', 'title']]\n",
    "\n",
    "#there are duplicates in the dataset, get the unique titles\n",
    "cluster1_titles = cluster1_titles.drop_duplicates()\n",
    "print(len(cluster1_titles))\n",
    "\n",
    "#save this as excel for readibility\n",
    "cluster1_titles.to_excel('cluster1_titles.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4038f-525f-4bf7-9a5f-a5aeb531f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same for clusters 5 and 7 \n",
    "docs_cluster_5_normalized = [s.replace('_', '/') for s in docs_cluster_5_clean]\n",
    "docs_cluster_7_normalized = [s.replace('_', '/') for s in docs_cluster_7_clean]\n",
    "\n",
    "cluster5_titles = doi_database[doi_database['doi'].isin(docs_cluster_5_normalized)][['doi', 'title']]\n",
    "cluster7_titles = doi_database[doi_database['doi'].isin(docs_cluster_7_normalized)][['doi', 'title']]\n",
    "\n",
    "cluster5_titles = cluster5_titles.drop_duplicates()\n",
    "cluster7_titles = cluster7_titles.drop_duplicates()\n",
    "print(len(cluster5_titles))\n",
    "print(len(cluster7_titles))\n",
    "\n",
    "#save this as excel for readibility\n",
    "cluster5_titles.to_excel('cluster5_titles.xlsx', index=False)\n",
    "cluster7_titles.to_excel('cluster7_titles.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
